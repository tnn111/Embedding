# Create_and_load_db

I would like you to create a new python script called create_and_load_db.

- The script should create a new ChromaDB persistent database and keep it it ".chroma" by default.
- The ChromaDB should use the 256-dimensional embedding generated by vae_encoder_best.keras to get embeddings for all sequences
  in Data/all_multimer_frequencies_l5000_shuffled.txt. It should use the IDs present in all_multimer_frequencies_l5000_shuffled.txt and it should add the length of the sequence as metadata.
- All of the sequences should be in a new collection called shrub_of_life.

Ask me any questions you need to please.

## 2025-11-29 Questions from Claude

1. **Input format**: The `.txt` file - is this the raw output from `calculate_kmer_frequencies` (ID + length + k-mers + GC), or a different format?

2. **Batch size**: The file likely has millions of sequences. Should I process them in batches (e.g., 10,000 at a time) to avoid memory issues?

3. **Existing database**: If `.chroma` already exists, should the script overwrite it, append to it, or error out?

4. **Progress output**: Do you want verbose progress messages (every N sequences) or just start/end?

## From Torben

The input format is the raw output from calculate_kmer_frequencies. The file should have close to 5,000,000 sequences and you should use whatever batch size you find useful. If .chroma already exists, the script should just fail for now. As to progress reports, one every 100,000 sequences would be good.

## 2025-11-29 23:23 - Database Created

Successfully loaded **4,776,770 sequences** into ChromaDB collection `shrub_of_life`.

- Database location: `.chroma`
- Embedding dimensions: 256 (VAE latent space)
- Distance metric: cosine similarity
- Batch size used: 5,000 (ChromaDB max is 5,461)
- Metadata: sequence length for each entry

---

## 2025-12-03 ~14:15: Updated for new data format

### Changes

Rewrote script to use new input format from `calculate_kmer_frequencies`:

**Old input:** Single text file with ID, length, and k-mer columns
**New input:** Two files:
- IDs file: Text file with one ID per line
- K-mers file: NumPy .npy with length at column 0, 6-mer through 1-mer frequencies at columns 1-2772

### New usage

```bash
./create_and_load_db -id Data/all_ids.txt -k Data/all_kmers.npy
```

### Other changes

- Now uses VAE encoder (`vae_encoder_best.keras`) with 384-dim latent space
- Applies CLR transformation before encoding
- Uses argparse for cleaner CLI
- Memory-maps .npy file for efficient batch processing