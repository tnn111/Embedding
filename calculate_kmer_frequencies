#!/usr/bin/env python3
"""
Calculate canonical k-mer frequencies from metagenomic contigs.

Reads one or more FASTA files and outputs:
- IDs file: Text file with one sequence ID per line
- K-mers file: NumPy array with length + canonical k-mer frequencies

Features per sequence (10,965 total):
- Sequence length (1 value)
- Normalized canonical 7-mer frequencies (8,192 values)
- Normalized canonical 6-mer frequencies (2,080 values)
- Normalized canonical 5-mer frequencies (512 values)
- Normalized canonical 4-mer frequencies (136 values)
- Normalized canonical 3-mer frequencies (32 values)
- Normalized canonical 2-mer frequencies (10 values)
- Normalized canonical 1-mer frequencies (2 values: A/T and C/G)

K-mers are canonical (lexicographically smallest of k-mer and reverse complement).
Each k-mer group is normalized separately to sum to 1.0.
"""

import argparse
import sys
from itertools import groupby, product
from pathlib import Path
from typing import Iterator

import numpy as np


def reverse_complement(seq: str) -> str:
    """Return the reverse complement of a DNA sequence."""
    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}
    return ''.join(complement[base] for base in reversed(seq))


def get_canonical_kmer(kmer: str) -> str:
    """Return the canonical form (lexicographically smallest) of a k-mer."""
    rc = reverse_complement(kmer)
    return min(kmer, rc)


def build_canonical_index(k: int) -> tuple[int, np.ndarray]:
    """
    Build array mapping integer k-mer encoding to canonical index.

    K-mer integer encoding: base[0]*4^(k-1) + base[1]*4^(k-2) + ... (big-endian base-4)
    Where A=0, C=1, G=2, T=3.

    Returns:
        tuple: (number of canonical k-mers, array mapping k-mer int -> canonical index)
    """
    bases = 'ACGT'
    base_to_int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}
    n_kmers = 4 ** k

    # Generate all k-mers and find canonical forms
    all_kmers = [''.join(kmer) for kmer in product(bases, repeat = k)]
    kmer_to_canonical = {kmer: get_canonical_kmer(kmer) for kmer in all_kmers}

    # Get sorted list of unique canonical k-mers
    canonical_list = sorted(set(kmer_to_canonical.values()))
    n_canonical = len(canonical_list)

    # Create mapping from canonical k-mer to its index
    canonical_to_idx = {kmer: i for i, kmer in enumerate(canonical_list)}

    # Build array: kmer_int -> canonical_index
    # Using big-endian encoding to match count_kmers sliding window
    kmer_int_to_idx = np.zeros(n_kmers, dtype = np.int32)
    for kmer in all_kmers:
        # Encode k-mer string to integer (big-endian: first base in high bits)
        kmer_int = 0
        for base in kmer:
            kmer_int = (kmer_int << 2) | base_to_int[base]
        canonical = kmer_to_canonical[kmer]
        kmer_int_to_idx[kmer_int] = canonical_to_idx[canonical]

    return n_canonical, kmer_int_to_idx


def count_kmers(seq_encoded: np.ndarray, k: int, canonical_map: np.ndarray, n_canonical: int) -> np.ndarray:
    """
    Count canonical k-mers in an encoded sequence using vectorized operations.

    Args:
        seq_encoded: Sequence encoded as integers (A=0, C=1, G=2, T=3)
        k: K-mer length
        canonical_map: Array mapping k-mer integers to canonical indices
        n_canonical: Number of canonical k-mers

    Returns:
        Counts array indexed by canonical k-mer position.
    """
    if len(seq_encoded) < k:
        return np.zeros(n_canonical, dtype = np.int32)

    # Build all k-mer integers using vectorized operations
    # Each position contributes: base * 4^(k-1-position_in_kmer)
    n_kmers = len(seq_encoded) - k + 1
    kmer_ints = np.zeros(n_kmers, dtype = np.int32)

    for i in range(k):
        kmer_ints += seq_encoded[i:i + n_kmers].astype(np.int32) << (2 * (k - 1 - i))

    # Map to canonical indices and count
    canonical_indices = canonical_map[kmer_ints]
    counts = np.bincount(canonical_indices, minlength = n_canonical).astype(np.int32)

    return counts


def read_fasta(fasta_path: Path) -> Iterator[tuple[str, str]]:
    """
    Yield (seq_id, sequence) tuples from a FASTA file.

    Handles multi-line sequences. Filters to ATGC only.
    """
    with open(fasta_path) as f:
        # Group lines by whether they start with '>'
        for is_header, group in groupby(f, lambda line: line.startswith('>')):
            if is_header:
                # Header line - extract ID (first word after '>')
                header = next(group).strip()
                seq_id = header[1:].split()[0] if header[1:].strip() else 'unknown'
            else:
                # Sequence lines - join and filter to ATGC
                seq = ''.join(line.strip() for line in group).upper()
                seq = ''.join(b for b in seq if b in 'ATGC')
                if seq:
                    yield seq_id, seq


def main():
    parser = argparse.ArgumentParser(description = 'Calculate canonical k-mer frequencies from FASTA files')
    parser.add_argument('-i', '--input', type = Path, nargs = '+', required = True,
                        help = 'One or more input FASTA files')
    parser.add_argument('-id', '--identifier', type = Path, required = True,
                        help = 'Output file for sequence IDs (text, one per line)')
    parser.add_argument('-k', '--kmer', type = Path, required = True,
                        help = 'Output file for k-mer frequencies (NumPy .npy format)')
    args = parser.parse_args()

    # Validate input files
    for f in args.input:
        if not f.exists():
            print(f'Error: File not found: {f}', file = sys.stderr)
            sys.exit(1)

    # Build canonical index mappings for k=1 through 7
    print('Building canonical k-mer mappings...', file = sys.stderr)
    k_data = {}
    for k in range(1, 8):
        n_canonical, mapping = build_canonical_index(k)
        k_data[k] = (n_canonical, mapping)
        print(f'  k={k}: {n_canonical} canonical k-mers', file = sys.stderr)

    # Calculate total features
    n_features = 1  # length
    for k in range(7, 0, -1):
        n_features += k_data[k][0]
    print(f'Total features per sequence: {n_features}', file = sys.stderr)

    # Encoding table: A=0, C=1, G=2, T=3
    base_to_int = np.zeros(256, dtype = np.uint8)
    base_to_int[ord('A')] = 0
    base_to_int[ord('C')] = 1
    base_to_int[ord('G')] = 2
    base_to_int[ord('T')] = 3

    # Collect all features in memory
    chunks = []
    CHUNK_SIZE = 100000
    chunk = np.zeros((CHUNK_SIZE, n_features), dtype = np.float32)
    chunk_idx = 0
    total_seqs = 0

    with open(args.identifier, 'w') as id_file:
        for input_file in args.input:
            print(f'Processing {input_file}...', file = sys.stderr)
            file_seqs = 0

            for seq_id, sequence in read_fasta(input_file):
                # Encode sequence
                seq_encoded = base_to_int[np.frombuffer(sequence.encode('ascii'), dtype = np.uint8)]

                # Fill row: length first, then k-mers from 7 down to 1
                col = 0
                chunk[chunk_idx, col] = len(sequence)
                col += 1

                for k in range(7, 0, -1):
                    n_canonical, mapping = k_data[k]
                    counts = count_kmers(seq_encoded, k, mapping, n_canonical)
                    total = counts.sum()
                    if total > 0:
                        chunk[chunk_idx, col:col + n_canonical] = counts / total
                    col += n_canonical

                # Write ID
                id_file.write(seq_id + '\n')

                chunk_idx += 1
                file_seqs += 1
                total_seqs += 1

                # Store chunk when full
                if chunk_idx >= CHUNK_SIZE:
                    chunks.append(chunk)
                    chunk = np.zeros((CHUNK_SIZE, n_features), dtype = np.float32)
                    chunk_idx = 0
                    print(f'  {total_seqs:,} sequences', file = sys.stderr)

            print(f'  Finished {input_file}: {file_seqs:,} sequences', file = sys.stderr)

    # Store remaining sequences
    if chunk_idx > 0:
        chunks.append(chunk[:chunk_idx])

    print(f'Total: {total_seqs:,} sequences', file = sys.stderr)

    # Concatenate and save
    if chunks:
        final = np.concatenate(chunks, axis = 0)
        np.save(args.kmer, final)
        print(f'Done! Output: {final.shape}', file = sys.stderr)
    else:
        print('No sequences processed.', file = sys.stderr)


if __name__ == '__main__':
    main()
