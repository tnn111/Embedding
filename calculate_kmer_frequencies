#!/usr/bin/env python
"""
Calculate canonical k-mer frequencies from metagenomic contigs.

Reads one or more FASTA files (optionally gzip-compressed) and outputs:
- IDs file: Text file with one sequence ID per line
- K-mers file: NumPy array with length + canonical k-mer frequencies

Features per sequence (2,773 total):
- Sequence length (1 value)
- Normalized canonical 6-mer frequencies (2,080 values)
- Normalized canonical 5-mer frequencies (512 values)
- Normalized canonical 4-mer frequencies (136 values)
- Normalized canonical 3-mer frequencies (32 values)
- Normalized canonical 2-mer frequencies (10 values)
- Normalized canonical 1-mer frequencies (2 values: A/T and C/G)

K-mers are canonical (lexicographically smallest of k-mer and reverse complement).
Each k-mer group is normalized separately to sum to 1.0.
"""

import argparse
import gzip
import sys
import tempfile
from itertools import groupby, product
from pathlib import Path
from typing import Iterator

import numpy as np


def reverse_complement(seq: str) -> str:
    """Return the reverse complement of a DNA sequence."""
    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}
    return ''.join(complement[base] for base in reversed(seq))


def get_canonical_kmer(kmer: str) -> str:
    """Return the canonical form (lexicographically smallest) of a k-mer."""
    rc = reverse_complement(kmer)
    return min(kmer, rc)


def build_canonical_index(k: int) -> tuple[int, np.ndarray]:
    """
    Build array mapping integer k-mer encoding to canonical index.

    K-mer integer encoding: base[0]*4^(k-1) + base[1]*4^(k-2) + ... (big-endian base-4)
    Where A=0, C=1, G=2, T=3.

    Returns:
        tuple: (number of canonical k-mers, array mapping k-mer int -> canonical index)
    """
    bases = 'ACGT'
    base_to_int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}
    n_kmers = 4 ** k

    # Generate all k-mers and find canonical forms
    all_kmers = [''.join(kmer) for kmer in product(bases, repeat = k)]
    kmer_to_canonical = {kmer: get_canonical_kmer(kmer) for kmer in all_kmers}

    # Get sorted list of unique canonical k-mers
    canonical_list = sorted(set(kmer_to_canonical.values()))
    n_canonical = len(canonical_list)

    # Create mapping from canonical k-mer to its index
    canonical_to_idx = {kmer: i for i, kmer in enumerate(canonical_list)}

    # Build array: kmer_int -> canonical_index
    # Using big-endian encoding to match count_kmers sliding window
    kmer_int_to_idx = np.zeros(n_kmers, dtype = np.int32)
    for kmer in all_kmers:
        # Encode k-mer string to integer (big-endian: first base in high bits)
        kmer_int = 0
        for base in kmer:
            kmer_int = (kmer_int << 2) | base_to_int[base]
        canonical = kmer_to_canonical[kmer]
        kmer_int_to_idx[kmer_int] = canonical_to_idx[canonical]

    return n_canonical, kmer_int_to_idx


def count_kmers(seq_encoded: np.ndarray, k: int, canonical_map: np.ndarray, n_canonical: int) -> np.ndarray:
    """
    Count canonical k-mers in an encoded sequence using vectorized operations.

    Args:
        seq_encoded: Sequence encoded as integers (A=0, C=1, G=2, T=3)
        k: K-mer length
        canonical_map: Array mapping k-mer integers to canonical indices
        n_canonical: Number of canonical k-mers

    Returns:
        Counts array indexed by canonical k-mer position.
    """
    if len(seq_encoded) < k:
        return np.zeros(n_canonical, dtype = np.int32)

    # Build all k-mer integers using vectorized operations
    # Each position contributes: base * 4^(k-1-position_in_kmer)
    n_kmers = len(seq_encoded) - k + 1
    kmer_ints = np.zeros(n_kmers, dtype = np.int32)

    for i in range(k):
        kmer_ints += seq_encoded[i:i + n_kmers].astype(np.int32) << (2 * (k - 1 - i))

    # Map to canonical indices and count
    canonical_indices = canonical_map[kmer_ints]
    counts = np.bincount(canonical_indices, minlength = n_canonical).astype(np.int32)

    return counts


def read_fasta(fasta_path: Path) -> Iterator[tuple[str, str]]:
    """
    Yield (seq_id, sequence) tuples from a FASTA file.

    Handles multi-line sequences and gzip-compressed files (.gz).
    Filters to ATGC only.
    """
    opener = gzip.open if fasta_path.suffix == '.gz' else open
    with opener(fasta_path, 'rt') as f:
        # Group lines by whether they start with '>'
        for is_header, group in groupby(f, lambda line: line.startswith('>')):
            if is_header:
                # Header line - extract ID (first word after '>')
                header = next(group).strip()
                seq_id = header[1:].split()[0] if header[1:].strip() else 'unknown'
            else:
                # Sequence lines - join and filter to ATGC
                seq = ''.join(line.strip() for line in group).upper()
                seq = ''.join(b for b in seq if b in 'ATGC')
                if seq:
                    yield seq_id, seq


def main():
    parser = argparse.ArgumentParser(description = 'Calculate canonical k-mer frequencies from FASTA files')
    parser.add_argument('-i', '--input', type = Path, nargs = '+', required = True,
                        help = 'One or more input FASTA files')
    parser.add_argument('-id', '--identifier', type = Path, required = True,
                        help = 'Output file for sequence IDs (text, one per line)')
    parser.add_argument('-k', '--kmer', type = Path, required = True,
                        help = 'Output file for k-mer frequencies (NumPy .npy format)')
    args = parser.parse_args()

    # Validate input files
    for f in args.input:
        if not f.exists():
            print(f'Error: File not found: {f}', file = sys.stderr)
            sys.exit(1)

    # Build canonical index mappings for k=1 through 6 (no 7-mers)
    print('Building canonical k-mer mappings...', file = sys.stderr)
    k_data = {}
    for k in range(1, 7):
        n_canonical, mapping = build_canonical_index(k)
        k_data[k] = (n_canonical, mapping)
        print(f'  k={k}: {n_canonical} canonical k-mers', file = sys.stderr)

    # Calculate total features
    n_features = 1  # length
    for k in range(6, 0, -1):
        n_features += k_data[k][0]
    print(f'Total features per sequence: {n_features}', file = sys.stderr)

    # Encoding table: A=0, C=1, G=2, T=3
    base_to_int = np.zeros(256, dtype = np.uint8)
    base_to_int[ord('A')] = 0
    base_to_int[ord('C')] = 1
    base_to_int[ord('G')] = 2
    base_to_int[ord('T')] = 3

    # Process sequences, writing chunks to temp files to minimize memory usage
    CHUNK_SIZE = 100000
    chunk = np.zeros((CHUNK_SIZE, n_features), dtype = np.float32)
    chunk_idx = 0
    total_seqs = 0
    temp_files = []
    temp_dir = tempfile.mkdtemp(prefix = 'kmer_chunks_')

    print(f'Using temp directory: {temp_dir}', file = sys.stderr)

    try:
        with open(args.identifier, 'w') as id_file:
            for input_file in args.input:
                print(f'Processing {input_file}...', file = sys.stderr)
                file_seqs = 0

                for seq_id, sequence in read_fasta(input_file):
                    # Encode sequence
                    seq_encoded = base_to_int[np.frombuffer(sequence.encode('ascii'), dtype = np.uint8)]

                    # Fill row: length first, then k-mers from 6 down to 1
                    col = 0
                    chunk[chunk_idx, col] = len(sequence)
                    col += 1

                    for k in range(6, 0, -1):
                        n_canonical, mapping = k_data[k]
                        counts = count_kmers(seq_encoded, k, mapping, n_canonical)
                        total = counts.sum()
                        if total > 0:
                            chunk[chunk_idx, col:col + n_canonical] = counts / total
                        col += n_canonical

                    # Write ID
                    id_file.write(seq_id + '\n')

                    chunk_idx += 1
                    file_seqs += 1
                    total_seqs += 1

                    # Write chunk to temp file when full
                    if chunk_idx >= CHUNK_SIZE:
                        temp_path = Path(temp_dir) / f'chunk_{len(temp_files):04d}.npy'
                        np.save(temp_path, chunk)
                        temp_files.append(temp_path)
                        chunk = np.zeros((CHUNK_SIZE, n_features), dtype = np.float32)
                        chunk_idx = 0
                        print(f'  {total_seqs:,} sequences (wrote chunk {len(temp_files)})', file = sys.stderr)

                print(f'  Finished {input_file}: {file_seqs:,} sequences', file = sys.stderr)

        # Write remaining sequences
        if chunk_idx > 0:
            temp_path = Path(temp_dir) / f'chunk_{len(temp_files):04d}.npy'
            np.save(temp_path, chunk[:chunk_idx])
            temp_files.append(temp_path)

        print(f'Total: {total_seqs:,} sequences in {len(temp_files)} chunks', file = sys.stderr)

        # Concatenate using memmap to minimize memory
        if temp_files:
            print('Concatenating chunks to output file...', file = sys.stderr)

            # Create output memmap
            output = np.lib.format.open_memmap(
                args.kmer, mode = 'w+', dtype = np.float32, shape = (total_seqs, n_features)
            )

            # Copy chunks one at a time
            row = 0
            for i, temp_path in enumerate(temp_files):
                chunk_data = np.load(temp_path)
                output[row:row + len(chunk_data)] = chunk_data
                row += len(chunk_data)
                if (i + 1) % 10 == 0:
                    print(f'  Copied {i + 1}/{len(temp_files)} chunks', file = sys.stderr)

            # Flush to disk
            del output

            print(f'Done! Output: ({total_seqs}, {n_features})', file = sys.stderr)
        else:
            print('No sequences processed.', file = sys.stderr)

    finally:
        # Clean up temp files
        print('Cleaning up temp files...', file = sys.stderr)
        for temp_path in temp_files:
            try:
                temp_path.unlink()
            except OSError:
                pass
        try:
            Path(temp_dir).rmdir()
        except OSError:
            pass


if __name__ == '__main__':
    main()
