#!/usr/bin/env -S uv run --quiet --script
"""
Create and load a ChromaDB database with VAE embeddings.

Reads k-mer frequency data from calculate_kmer_frequencies output,
generates embeddings using the VAE encoder, and stores them in ChromaDB.

Usage:
    ./create_and_load_db Data/all_multimer_frequencies_l5000_shuffled.txt
"""
# /// script
# dependencies = [
#   "numpy",
#   "keras",
#   "jax[cuda12]",
#   "chromadb",
# ]
# ///

import sys
import os
os.environ['KERAS_BACKEND'] = 'jax'
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'

import numpy as np
import keras
import chromadb


# Custom layers required for loading the encoder
SEED = 42


@keras.saving.register_keras_serializable()
class ClipLayer(keras.layers.Layer):
    """Clips tensor values to a specified range."""

    def __init__(self, min_value = -20, max_value = 2, **kwargs):
        super().__init__(**kwargs)
        self.min_value = min_value
        self.max_value = max_value

    def call(self, inputs):
        return keras.ops.clip(inputs, self.min_value, self.max_value)

    def get_config(self):
        config = super().get_config()
        config.update({'min_value': self.min_value, 'max_value': self.max_value})
        return config


@keras.saving.register_keras_serializable()
class Sampling(keras.layers.Layer):
    """Reparameterization trick: sample z = mean + exp(log_var/2) * epsilon."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.seed_generator = keras.random.SeedGenerator(SEED)

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = keras.ops.shape(z_mean)[0]
        dim = keras.ops.shape(z_mean)[1]
        epsilon = keras.random.normal(shape = (batch, dim), seed = self.seed_generator)
        return z_mean + keras.ops.exp(0.5 * z_log_var) * epsilon

    def get_config(self):
        return super().get_config()


CHROMA_PATH = '.chroma'
COLLECTION_NAME = 'shrub_of_life'
ENCODER_PATH = 'vae_encoder_final.keras'
BATCH_SIZE = 5000
PROGRESS_INTERVAL = 100000


def main():
    if len(sys.argv) < 2:
        print('Usage: ./create_and_load_db <input_file.txt>', file = sys.stderr)
        sys.exit(1)

    input_file = sys.argv[1]

    # Check if database already exists
    if os.path.exists(CHROMA_PATH):
        print(f'Error: {CHROMA_PATH} already exists. Remove it first.', file = sys.stderr)
        sys.exit(1)

    # Load encoder
    print('Loading encoder...', file = sys.stderr)
    encoder = keras.saving.load_model(
        ENCODER_PATH,
        custom_objects = {'ClipLayer': ClipLayer, 'Sampling': Sampling}
    )

    # Create ChromaDB client and collection
    print(f'Creating ChromaDB at {CHROMA_PATH}...', file = sys.stderr)
    client = chromadb.PersistentClient(path = CHROMA_PATH)
    collection = client.create_collection(
        name = COLLECTION_NAME,
        metadata = {'hnsw:space': 'cosine'}
    )

    # Process input file in batches
    print(f'Processing {input_file}...', file = sys.stderr)

    batch_ids = []
    batch_lengths = []
    batch_features = []
    total_processed = 0

    with open(input_file, 'r') as f:
        for line in f:
            fields = line.strip().split()
            seq_id = fields[0]
            length = int(fields[1])
            # Skip ID and length, keep k-mer features (columns 2-2762)
            features = np.array([float(x) for x in fields[2:]], dtype = np.float32)

            batch_ids.append(seq_id)
            batch_lengths.append(length)
            batch_features.append(features)

            if len(batch_ids) >= BATCH_SIZE:
                # Process batch
                features_array = np.array(batch_features, dtype = np.float32)
                _, _, z = encoder.predict(features_array, batch_size = BATCH_SIZE, verbose = 0)

                # Add to ChromaDB
                collection.add(
                    ids = batch_ids,
                    embeddings = z.tolist(),
                    metadatas = [{'length': l} for l in batch_lengths]
                )

                total_processed += len(batch_ids)
                if total_processed % PROGRESS_INTERVAL == 0:
                    print(f'Processed {total_processed:,} sequences...', file = sys.stderr)

                # Clear batch
                batch_ids = []
                batch_lengths = []
                batch_features = []

    # Process remaining sequences
    if batch_ids:
        features_array = np.array(batch_features, dtype = np.float32)
        _, _, z = encoder.predict(features_array, batch_size = len(batch_ids), verbose = 0)

        collection.add(
            ids = batch_ids,
            embeddings = z.tolist(),
            metadatas = [{'length': l} for l in batch_lengths]
        )

        total_processed += len(batch_ids)

    print(f'Done. Loaded {total_processed:,} sequences into {COLLECTION_NAME}.', file = sys.stderr)


if __name__ == '__main__':
    main()
