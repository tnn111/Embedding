#!/usr/bin/env -S uv run --quiet --script
"""
Create and load a ChromaDB database with VAE embeddings.

Reads k-mer frequency data from calculate_kmer_frequencies output (.npy format),
generates embeddings using the VAE encoder, and stores them in ChromaDB.

Input files:
    - IDs: Text file with one sequence ID per line (Data/all_ids.txt)
    - K-mers: NumPy .npy file with length + 6-mer through 1-mer frequencies (Data/all_kmers.npy)

Usage:
    ./create_and_load_db -id Data/all_ids.txt -k Data/all_kmers.npy
"""
# /// script
# dependencies = [
#   "numpy",
#   "keras",
#   "jax[cuda12]",
#   "chromadb",
# ]
# ///

import sys
import os
os.environ['KERAS_BACKEND'] = 'jax'
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'

import numpy as np
import keras
import chromadb


# Custom layers required for loading the encoder
SEED = 42


@keras.saving.register_keras_serializable()
class ClipLayer(keras.layers.Layer):
    """Clips tensor values to a specified range."""

    def __init__(self, min_value = -20, max_value = 2, **kwargs):
        super().__init__(**kwargs)
        self.min_value = min_value
        self.max_value = max_value

    def call(self, inputs):
        return keras.ops.clip(inputs, self.min_value, self.max_value)

    def get_config(self):
        config = super().get_config()
        config.update({'min_value': self.min_value, 'max_value': self.max_value})
        return config


@keras.saving.register_keras_serializable()
class Sampling(keras.layers.Layer):
    """Reparameterization trick: sample z = mean + exp(log_var/2) * epsilon."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.seed_generator = keras.random.SeedGenerator(SEED)

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = keras.ops.shape(z_mean)[0]
        dim = keras.ops.shape(z_mean)[1]
        epsilon = keras.random.normal(shape = (batch, dim), seed = self.seed_generator)
        return z_mean + keras.ops.exp(0.5 * z_log_var) * epsilon

    def get_config(self):
        return super().get_config()


CHROMA_PATH = '.chroma'
COLLECTION_NAME = 'shrub_of_life'
ENCODER_PATH = 'vae_encoder_best.keras'
BATCH_SIZE = 5000
PROGRESS_INTERVAL = 100000

# Column ranges in k-mers.npy (new format: col 0 = length, cols 1-2772 = k-mers)
COL_LENGTH = 0
COL_KMER_START = 1
COL_KMER_END = 2773
INPUT_DIM = 2772


def clr_transform(data: np.ndarray, pseudocount: float = 1e-6) -> np.ndarray:
    """Apply Centered Log-Ratio (CLR) transformation.

    CLR is appropriate for compositional data like k-mer frequencies.
    CLR(x_i) = log(x_i / geometric_mean(x))
    """
    data = data + pseudocount
    log_data = np.log(data)
    log_geom_mean = np.mean(log_data, axis = 1, keepdims = True)
    return log_data - log_geom_mean


def main():
    import argparse

    parser = argparse.ArgumentParser(description = 'Create and load ChromaDB with VAE embeddings')
    parser.add_argument('-id', '--ids', required = True, help = 'Path to IDs file (one ID per line)')
    parser.add_argument('-k', '--kmers', required = True, help = 'Path to k-mers .npy file')
    parser.add_argument('-e', '--encoder', default = ENCODER_PATH, help = f'Path to encoder model (default: {ENCODER_PATH})')
    parser.add_argument('-d', '--db', default = CHROMA_PATH, help = f'Path to ChromaDB directory (default: {CHROMA_PATH})')
    args = parser.parse_args()

    # Check if database already exists
    if os.path.exists(args.db):
        print(f'Error: {args.db} already exists. Remove it first.', file = sys.stderr)
        sys.exit(1)

    # Load encoder
    print(f'Loading encoder from {args.encoder}...', file = sys.stderr)
    encoder = keras.saving.load_model(
        args.encoder,
        custom_objects = {'ClipLayer': ClipLayer, 'Sampling': Sampling}
    )

    # Load IDs
    print(f'Loading IDs from {args.ids}...', file = sys.stderr)
    with open(args.ids, 'r') as f:
        all_ids = [line.strip() for line in f]
    print(f'Loaded {len(all_ids):,} IDs', file = sys.stderr)

    # Memory-map k-mer data
    print(f'Loading k-mers from {args.kmers}...', file = sys.stderr)
    data_mmap = np.load(args.kmers, mmap_mode = 'r')
    n_sequences = len(data_mmap)
    print(f'Found {n_sequences:,} sequences with {data_mmap.shape[1]} columns', file = sys.stderr)

    if len(all_ids) != n_sequences:
        print(f'Error: ID count ({len(all_ids)}) != sequence count ({n_sequences})', file = sys.stderr)
        sys.exit(1)

    # Create ChromaDB client and collection
    print(f'Creating ChromaDB at {args.db}...', file = sys.stderr)
    client = chromadb.PersistentClient(path = args.db)
    collection = client.create_collection(
        name = COLLECTION_NAME,
        metadata = {'hnsw:space': 'cosine'}
    )

    # Process in batches
    total_processed = 0

    for batch_start in range(0, n_sequences, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, n_sequences)
        batch_size = batch_end - batch_start

        # Get batch IDs
        batch_ids = all_ids[batch_start:batch_end]

        # Get batch lengths (column 0)
        batch_lengths = data_mmap[batch_start:batch_end, COL_LENGTH].astype(int).tolist()

        # Get batch k-mer features (columns 1-2772) and apply CLR transform
        batch_features = data_mmap[batch_start:batch_end, COL_KMER_START:COL_KMER_END].astype(np.float32)
        batch_features = clr_transform(batch_features)

        # Generate embeddings
        _, _, z = encoder.predict(batch_features, batch_size = batch_size, verbose = 0)

        # Add to ChromaDB
        collection.add(
            ids = batch_ids,
            embeddings = z.tolist(),
            metadatas = [{'length': l} for l in batch_lengths]
        )

        total_processed += batch_size
        if total_processed % PROGRESS_INTERVAL == 0:
            print(f'Processed {total_processed:,} sequences...', file = sys.stderr)

    print(f'Done. Loaded {total_processed:,} sequences into {COLLECTION_NAME}.', file = sys.stderr)


if __name__ == '__main__':
    main()
